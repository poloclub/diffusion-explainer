d3.select("#description")
    .append("div")
        .attr("id", "description-top-button-container")
        .on("click", function() {
            document.body.scrollTop = 0; // For Safari
            document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
        })
d3.select("#description-top-button-container")
    .append("img")
        .attr("src", "icons/top.svg")
        .attr("id", "description-top-button-img")
d3.select("#description-top-button-container")
    .append("div")
        .attr("id", "description-top-button-text")
        .text("top")
d3.select("#description")
    .append("div")
        .attr("id", "description-section-what")
        .attr("class", "description-sec")
        .append("h1")
            .text("What is Stable Diffusion?")
d3.select("#description-section-what")
    .append("p")
        .html(`
        Stable Diffusion is a text-to-image model that transforms a text prompt into a high-resolution image. 
        For example, if you type in 
        <span style="color: var(--text3);">a cute and adorable bunny</span>, 
        Stable Diffusion generates high-resolution images depicting that 
        &mdash; <span style="color: var(--text3);">a cute and adorable bunny</span> &mdash; 
        in a few seconds. 
        Click “Select another prompt” in Diffusion Explainer to change prompts and check the fascinating images generated from each prompt!`)

// How does Stable Diffusion work?
d3.select("#description")
    .append("div")
        .attr("id", "description-section-how-work")
        .attr("class", "description-sec")
        .append("h1")
            .text("How does Stable Diffusion work?")
d3.select("#description-section-how-work")
    .append("p")
        // .text('Stable Diffusion first generates a vector representation of an image depicted in the text prompt. This image representation is then upscaled into a high-resolution image.')
        .html(`
        Stable Diffusion first changes the text prompt into a <span style="font-style: italic">text representation</span>, 
        numerical values that summarize the prompt. 
        The text representation is used to generate an <span style="font-style: italic">image representation</span>, 
        which summarizes an image depicted in the text prompt. 
        This image representation is then upscaled into a high-resolution image.
        `)
d3.select("#description-section-how-work")
    .append("p")
        // .html('You may wonder why Stable Diffusion introduces image representation instead of directly generating high-resolution images. The reason is <span style="font-style: italic;">computational cost efficiency</span>. Doing most computations on representation, which summarizes an image in a compact form, significantly reduces the computational cost while maintaining high image quality.')
        .html(`
        You may wonder why Stable Diffusion introduces image representation instead of directly generating high-resolution images. 
        The reason is <span style="font-style: italic">computational efficiency</span>. 
        Doing most computations on compact image representation instead of a high-resolution image significantly reduces the time and cost for the computations while maintaining high image quality.
        `)
d3.select("#description-section-how-work")
    .append("p")
        // .html('The image representation starts as a random noise and is refined over multiple timesteps to reach the image representation for your text prompt. The number of timesteps is a hyperparameter determined before refining and typically set to 50.')
        .html(`
        The image representation, which starts as a random noise, 
        is refined over multiple timesteps to reach the image representation for a high-quality image with strong adherence to the text prompt. 
        The number of refining timesteps is typically set as 50 or 100; we fix it to 50 in Diffusion Explainer.
        `)
d3.select("#description-section-how-work")
    .append("p")
        .html('We break down the image generation process of Stable Diffusion into three main steps:')
        .append("ol")
        .attr("id", "description-generation-main-steps-ol")
d3.select("#description-generation-main-steps-ol")
    .append("li")
        .html('<a style="font-weight: 500" href="#description-subsec-text-representation-generation">Text Representation Generation</a>: Stable Diffusion converts a text prompt into a text vector representation.')
d3.select("#description-generation-main-steps-ol")
    .append("li")
        .html('<a style="font-weight: 500" href="#description-subsec-image-representation-refining">Image Representation Refining</a>: Starting with random noise, Stable Diffusion refines the image representation little by little, with the guidance of the text representation. Stable Diffusion repeats the refining over multiple timesteps (50 in our Diffusion Explainer).')
d3.select("#description-generation-main-steps-ol")
    .append("li")
        .html('<a style="font-weight: 500" href="#description-subsec-image-upscaling">Image Upscaling</a>: Stable Diffusion upscales the image representation into a high-resolution image.')
d3.select("#description-section-how-work")
    .append("p")
        .text("Now, let's look closer into each process.")

// Text Representation Generation
d3.select("#description-section-how-work")
    .append("div")
        .attr("id", "description-subsec-text-representation-generation")
        .attr("class", "description-subsec")
        .append("h2")
            .text("Text Representation Generation")
d3.select("#description-subsec-text-representation-generation")
    .append("img")
        .attr("class", "description-gif")
        .attr("id", "text-representation-generation-expansion-gif")
        .attr("src", "assets/gif/trg.gif")
d3.select("#description-subsec-text-representation-generation")
    .append("p")
        // .text("Text representation generation consists of tokenizing and text encoding.")
        .html(`
        Clicking Text Representation Generation shows how a text prompt is converted into a text representation, 
        a vector that summarizes the prompt. 
        It consists of two steps: 
        <span style="font-style: italic">tokenizing</span> 
        and 
        <span style="font-style: italic">text encoding</span>.`)
// Tokenizing
d3.select("#description-subsec-text-representation-generation")
    .append("div")
        .attr("id", "description-subsubsec-tokenizing")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-tokenizing")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('1. Tokenizing')
d3.select("#description-subsubsec-tokenizing")
    .append("p")
        // .html("Tokenizing is a common way to handle text input to standardize the format of the input and enable the text input to be processed by neural networks.")
        .html(`Tokenizing is a common way to handle text data to change the text into numbers and process them with neural networks.`)
d3.select("#description-subsubsec-tokenizing")
    .append("div")
        .attr("class", "description-paragraph")
        .attr("id", "description-subsubsec-tokenizing-token-example-paragraph")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
    .html(`Stable Diffusion tokenizes a text prompt into a sequence of tokens. 
    For example, it splits the text prompt `)
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .style("color", "var(--text3)")
        .text("a cute and adorable bunny ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text("into the tokens ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("a")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("cute")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("and")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("adorable")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", and ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("bunny")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(". Also, to mark the beginning and end of the prompt, Stable Diffusion adds ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<start>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(" and ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<end>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(" tokens at the beginning and the end of the tokens. The resulting token sequence for the above example would be ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<start>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("a")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("cute")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("and")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("adorable")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("bunny")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", and ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<end>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(".")

d3.select("#description-subsubsec-tokenizing")
    .append("p")
        // .html('To ensure that all token sequences have the same length for easier computation, Stable Diffusion pads or truncates the token sequences to exactly 77 tokens. If the input prompt has fewer than 77 tokens, <span class="text-vector-generator-token description-token" id="description-token-end"></span> tokens are added to the end of the sequence until it reaches 77 tokens. If the input prompt has more than 77 tokens, the last 77 tokens are retained and the rest are truncated. The number of tokens was set to balance performance and computational efficiency.')
        .html('For easier computation, Stable Diffusion keeps the token sequences of any text prompts to have the same length of 77 by padding or truncating. If the input prompt has fewer than 77 tokens, <span class="text-vector-generator-token description-token" id="description-token-end"></span> tokens are added to the end of the sequence until it reaches 77 tokens. If the input prompt has more than 77 tokens, the first 77 tokens are retained and the rest are truncated. The length of 77 was set to balance performance and computational efficiency.')
d3.select("#description-token-end").text("<end>")
// Text encoding
d3.select("#description-subsec-text-representation-generation")
    .append("div")
        .attr("id", "description-subsubsec-text-encoding")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-text-encoding")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('2. Text encoding')
d3.select("#description-subsubsec-text-encoding")
    .append("p")
        .html(`Stable Diffusion converts the token sequence into a text representation. To use the text representation for guiding image generation, Stable Diffusion ensures that the text representation contains the information related to the image depicted in the prompt. This is done by using a special neural network called <a href="https://openai.com/research/clip">CLIP</a>.`)
d3.select("#description-subsubsec-text-encoding")
    .append("p")
        .html("CLIP, which consists of an image encoder and a text encoder, is trained to encode an image and its text description into vectors that are similar to each other. Therefore, the text representation for a prompt computed by CLIP’s text encoder is likely to contain information about the images described in the prompt. You can display the visual explanations by clicking the Text Encoder above.")

// Image Representation Refining
d3.select("#description-section-how-work")
    .append("div")
        .attr("id", "description-subsec-image-representation-refining")
        .attr("class", "description-subsec")
        .append("h2")
            .text("Image Representation Refining")
d3.select("#description-subsec-image-representation-refining")
    .append("img")
        .attr("class", "description-gif")
        .attr("id", "image-refining-description-gif")
        .attr("src", "assets/gif/irr.gif")
d3.select("#description-subsec-image-representation-refining")
    .append("p")
        .html("Stable Diffusion generates image representation, a vector that numerically summarizes a high-resolution image depicted in the text prompt. This is done by refining a randomly initialized noise over multiple timesteps to gradually improve the image quality and adherence to the prompt. You can change the initial random noise by adjusting the <span style='font-style: italic;'>seed </span> in Diffusion Explainer. Click Image Representation Refiner to visualize each refinement step, which involves noise prediction and removal.")
// Noise Prediction
d3.select("#description-subsec-image-representation-refining")
    .append("div")
        .attr("id", "description-subsubsec-noise-prediction")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-noise-prediction")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('1. Noise Prediction')
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .html("At each timestep, a neural network called UNet predicts noise in the image representation of the current timestep. UNet takes three inputs:")
d3.select("#description-subsubsec-noise-prediction")
    .append("ol")
        .attr("id", "description-unet-input-ol")
d3.select("#description-unet-input-ol")
    .append("li")
    .html(`<span style="font-weight: 500;">Image representation</span> of the current timestep`)
d3.select("#description-unet-input-ol")
    .append("li")
    .html(`<span style="font-weight: 500; color: var(--text3);">Text representation</span> of the prompt to guide what noise should be removed from the current image representation to generate an image adhering to the text prompt`)
d3.select("#description-unet-input-ol")
    .append("li")
    .html(`<span style="font-weight: 500;">Timestep</span> to indicate the amount of noise remaining in the current image representation`)

d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .html(`In other words, 
        UNet predicts a <span style="color: var(--text3);">prompt-conditioned noise</span> in the current image representation 
        under the guidance of the text prompt's representation and timestep.`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .html(`However, even though we condition the noise prediction with the text prompt, 
        the generated image representation usually does not adhere strongly enough to the text prompt. 
        To improve the adherence, 
        Stable Diffusion measures the impact of the prompt by additionally predicting <span style="color: #909090;">generic noise conditioned on an empty prompt (" ")</span>
        and subtracting it from the prompt-conditioned noise:`) 
        // The final noise prediction is a weighted sum of the predicted 
        // <span style="color: #a0a0a0;">generic noise</span> and the
        // <span style="color: var(--text3);">prompt-conditioned noise</span>
        // with the weights controlled by the hyperparameter <span style="font-weight: 500;">guidance scale</span>:`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .attr("class", "description-equation")
        .html(`
        <span class="description-equation-term" style="color: var(--text3); background-color: #4d922110;">impact of prompt</span> 
        <span class="description-equation-op">=</span> 
        <span class="description-equation-term" style="color: var(--text3); background-color: #4d922110;">prompt-conditioned noise</span> 
        <span class="description-equation-op">-</span> 
        <span class="description-equation-term" style="color: #909090; background-color: #a0a0a020;">generic noise</span>`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
    .html(`
    In other words, the generic noise contributes to better image quality, 
    while the impact of the prompt contributes to the adherence to the prompt. 
    The final noise is a weighted sum of them controlled by a value called <span style="color: var(--text3);">guidance scale</span>:`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .attr("class", "description-equation")
        .attr("id", "description-equation-gs")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .style("color", "#909090")
        .text("generic noise")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-op")
        .text(" + ")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#27641910")
        .style("color", "var(--text3)")
        .text("guidance scale")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-op")
        .text(" x ")
d3.select("#description-equation-gs")
    .append("span")
        .style("background-color", "#27641910")
        .style("color", "var(--text3)")
        .attr("class", "description-equation-term")
        .text("impact of prompt")
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
    .html(`A guidance scale of 0 means no adherence to the text prompt, 
    while a guidance scale of 1 means using the original prompt-conditioned noise. 
    Larger guidance scales result in stronger adherence to the text prompt, 
    while too large values can lower the image quality. 
    Change the guidance scale value in Diffusion Explainer and see how it changes the generated images.`)

// Noise Removal
d3.select("#description-subsec-image-representation-refining")
    .append("div")
        .attr("id", "description-subsubsec-noise-removal")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-noise-removal")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('2. Noise Removal')
d3.select("#description-subsubsec-noise-removal")
    .append("p")
        .html("Stable Diffusion then decides how much of the predicted noise to actually remove from the image, as determined by an algorithm called scheduler. Removing small amounts of noise helps refine the image gradually and produce sharper images.")
d3.select("#description-subsubsec-noise-removal")
    .append("p")
        .html("The scheduler makes this decision by accounting for the total number of timesteps. The downscaled noise is then subtracted from the image representation of the current timestep to obtain the refined representation, which becomes the image representation of the next timestep:")
d3.select("#description-subsubsec-noise-removal")
    .append("p")
        .attr("class", "description-equation")
        .attr("id", "description-equation-denoise")
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .html(`image representation of timestep <span style="font-style: italic;">t+1</span>`)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-op")
        .html(` = `)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .html(`image representation of timestep <span style="font-style: italic;">t</span>`)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-op")
        .html(` - `)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .html(`downscaled noise`)

// Image Upscaling
d3.select("#description-section-how-work")
    .append("div")
        .attr("id", "description-subsec-image-upscaling")
        .attr("class", "description-subsec")
            .append("h2")
                .text("Image Upscaling")
d3.select("#description-subsec-image-upscaling")
    .append("img")
    .attr("class", "description-gif")
    .attr("src", "assets/gif/upscale.gif")
d3.select("#description-subsec-image-upscaling")
    .append("p")
        .text("After all denoising steps have been completed, Stable Diffusion uses a neural network called Decoder to upscale the image representation into a high-resolution image. The refined image representation fully denoised with the guidance of the text representations would result in a high-resolution image strongly adhering to the text prompt.")

// Comparison View
d3.select("#description")
    .append("div")
        .attr("id", "description-section-comparison")
        .attr("class", "description-sec")
        .append("h1")
            .text("How do prompt keywords affect image generation?")
            .style("margin-bottom", "0.5em")
d3.select("#description-section-comparison")
    .append("img")
        .attr("class", "description-gif")
        .attr("id", "rcv-expansion-gif")
        .attr("src", "assets/gif/rcv.gif")
d3.select("#description-section-comparison")
    .append("p")
        .html(`
        Writing text prompts can be very heuristic and repetitive. 
        For example, starting from the prompt 
        <span style="color: var(--text3); font-style: italic;">a cute bunny</span>, 
        you should repetitively add and remove keywords such as 
        <span style="color: var(--text3); font-style: italic;">in the style of cute pixar character</span>, 
        until you reach to the desired image.`)
d3.select("#description-section-comparison")
    .append("p")
        .text(`
        Therefore, understanding how prompt keywords affect image generation would be greatly helpful for writing and refining your prompt. 
        Click the keywords highlighted in the text prompt and compare the image generation of the two prompts that differ only in the keywords.`)

// What can we change
d3.select("#description")
    .append("div")
        .attr("id", "description-section-change")
        .attr("class", "description-sec")
        .append("h1")
            .text("What can we change?")
d3.select("#description-section-change")
    .append("p")
        .text("You have control over text prompt and hyperparameters in our Diffusion Explainer to change the generated images:")
d3.select("#description-section-change")
    .append("ul")
        .attr("id", "description-hyperparameter-ol")
d3.select("#description-hyperparameter-ol")
    .append("li")
    .html(`Text prompt: Description of the image you want to generate. A more detailed text prompt generally leads to better quality images.`)
d3.select("#description-hyperparameter-ol")
    .append("li")
    .html(`Seed: Random seed for the initialization of the image representation at timestep 0. Changing the seed will result in different image representation at timestep 0 and therefore different images.`)
d3.select("#description-hyperparameter-ol")
    .append("li")
    .html(`Guidance scale: How closely the generated image adheres to the text prompt. Increasing the guidance scale leads to stronger adherence but may make the generated images overly exaggerated.`)
d3.select("#description-section-change")
    .append("p")
        .text("Additionally, there are other hyperparameters that are not included in the Diffusion Explainer, such as the total number of timesteps, image size, and the type of scheduler.")

// How implemented?
d3.select("#description")
    .append("div")
        .attr("id", "description-section-implement")
        .attr("class", "description-sec")
        .append("h1")
            .text("How is Diffusion Explainer implemented?")
d3.select("#description-section-implement")
    .append("p")
        .text("We implement the interactive visualizations for Diffusion Explainer using Javascript and D3.js.")

// How implemented?
d3.select("#description")
    .append("div")
        .attr("id", "description-section-who")
        .attr("class", "description-sec")
        .append("h1")
            .text("Who developed the Diffusion Explainer?")
d3.select("#description-section-who")
    .append("p")
        .html(`Diffusion Explainer was developed by 
        <a href="http://www.seongmin.xyz">Seongmin Lee</a>, 
        <a href="https://bhoov.com">Ben Hoover</a>, 
        <a href="http://hendrik.strobelt.com">Hendrik Strobelt</a>, 
        <a href="https://zijie.wang">Jay Wang</a>, 
        <a href="https://shengyun-peng.github.io">Anthony Peng</a>, 
        <a href="https://www.austinpwright.com">Austin Wright</a>, 
        <a href="https://www.linkedin.com/in/kevinyli/">Kevin Li</a>, 
        <a href="https://haekyu.com">Haekyu Park</a>, 
        <a href="https://alexanderyang.me">Alex Yang</a>, 
        and 
        <a href="https://poloclub.github.io/polochau/">Polo Chau</a>.`)